# ä»¥ä¸‹ã‚’ã€Œapp2.pyã€ã«æ›¸ãè¾¼ã¿
import streamlit as st
import openai
import os
from dotenv import load_dotenv
import platform
from pyngrok import ngrok
import chromadb
import langchain



from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader('ã¾ã¨ã‚2.pdf')
pages = loader.load_and_split()

load_dotenv()

# pipã‚’ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰
os.system('/home/adminuser/venv/bin/python -m pip install --upgrade pip')

# è¿½åŠ ã®é–‹ç™ºãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
os.system('/home/adminuser/venv/bin/python -m pip install python3-dev')

# openai.api_key = os.getenv("OPENAI_API_KEY")
openai.api_key =  os.environ['OPENAI_API_KEY']

# openai.api_key = st.secrets.OpenAIAPI.openai_api_key


# st.session_stateã‚’ä½¿ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã‚„ã‚Šã¨ã‚Šã‚’ä¿å­˜
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "system", "content": "ã‚ãªãŸã¯å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆAIã§ã™ã€‚"}
        ]

llm = ChatOpenAI(temperature=0, model_name="gpt-4")

embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(pages, embedding=embeddings, persist_directory=".")
vectorstore.persist()

# ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã¨ã‚„ã‚Šã¨ã‚Šã™ã‚‹é–¢æ•°
def communicate():
    # st.session_stateã‚’åˆæœŸåŒ–
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "user_input" not in st.session_state:
        st.session_state.user_input = ""

    pdf_qa = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)
    messages = st.session_state["messages"]

    user_message = {"role": "user", "content": st.session_state["user_input"]}
    messages.append(user_message)

    response = openai.ChatCompletion.create(
         model="gpt-4",
         messages=messages,
         temperature=0.0
     )
    query = user_message["content"]  # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ›ã‚’è³ªå•ã¨ã—ã¦ä½¿ç”¨
    chat_history = []

    result = pdf_qa({"question": query, "chat_history": chat_history})

    bot_message = result["answer"]
    messages.append({"role": "assistant", "content": bot_message})

    # bot_message = response["choices"][0]["message"]
    # messages.append(bot_message)

    st.session_state["user_input"] = ""  # å…¥åŠ›æ¬„ã‚’æ¶ˆå»


# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ã‚¤ã‚¹ã®æ§‹ç¯‰
st.title("AIæ¦‚è«–ã‚µãƒãƒ¼ãƒˆãƒãƒ£ãƒƒãƒˆã¸ã‚ˆã†ã“ã")
st.write("AIæ¦‚è«–ã‚µãƒãƒ¼ãƒˆãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚")
user_input = st.text_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚", key="user_input", on_change=communicate)
if  "messages" in st.session_state:
    messages = st.session_state["messages"]

    for message in reversed(messages[1:]):  # ç›´è¿‘ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¸Šã«
        speaker = "ğŸ™‚"
        if message["role"]=="assistant":
            speaker="ğŸ¤–"

        st.write(speaker + ": " + message["content"])
